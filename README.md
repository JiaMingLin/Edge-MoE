# Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts

Rishov Sarkar<sup>1</sup>, Hanxue Liang<sup>2</sup>, Zhiwen Fan<sup>2</sup>, Zhangyang Wang<sup>2</sup>, Cong Hao<sup>1</sup>

<sup>1</sup>School of Electrical and Computer Engineering, Georgia Institute of Technology  
<sup>2</sup>School of Electrical and Computer Engineering, University of Texas at Austin

## Overview

![Edge-MoE overall architecture](images/edge-moe-arch.svg)

This is **Edge-MoE**, the *first end-to-end* FPGA accelerator for *multi-task ViT* with a rich collection of architectural innovations.

Currently, this repository contains a prebuilt bitstream for the AMD/Xilinx ZCU102 FPGA and a video demo. Our HLS code will be open-source upon acceptance.
