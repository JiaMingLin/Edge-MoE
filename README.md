# Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts

Rishov Sarkar<sup>1</sup>, Hanxue Liang<sup>2</sup>, Zhiwen Fan<sup>2</sup>, Zhangyang Wang<sup>2</sup>, Cong Hao<sup>1</sup>

<sup>1</sup>School of Electrical and Computer Engineering, Georgia Institute of Technology  
<sup>2</sup>School of Electrical and Computer Engineering, University of Texas at Austin

ICCAD 2023 [paper](https://arxiv.org/abs/2305.18691)

## Overview

![Edge-MoE overall architecture](images/edge-moe-arch.svg)

This is **Edge-MoE**, the *first end-to-end* FPGA accelerator for *multi-task ViT* with a rich collection of architectural innovations.


